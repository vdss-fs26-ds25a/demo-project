{
  "hash": "615eee2f53a30a82c2c9a7e0d4c84de1",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Data Report\n---\n\n\nAll information on the data used in the project is compiled in the data report in order to ensure the traceability and reproducibility of the results and to enable a systematic expansion of the database.\n\nTypically, in the exploratory analysis of the acquired raw data, quality and other issues are identified, which require pre-processing, merging of individual datasets and feature engineering into processed datasets. Therefore, this template provides a separate section for the processed data, which then serves as a starting point for the modelling activities. This needs to be adapted to the specific project requirements.\n\n## Raw data\n### Overview Raw Datasets\n\n: Overview of raw datasets used in the project. {#tbl-raw-overview}\n\n| Name | Quelle | Storage location |\n|----------------|-----------------------------------------|--------------------------------------------------------------------------|\n| Dataset 1      | Name/short description of the data source | Link and/or short description of the location where the data is stored, e.g. accessible to the team |\n| Dataset 2      | …                                       | …                                                                        |\n\n### Details Dataset 1\n\n- Description of what information the dataset contains\n- Details of the data source/provider\n- Information on data procurement: description and possibly references to resources (download scripts, tools, online services, ...). Any new team member should be able to acquire the data indepentendently following these instructions.\n- Legal aspects of data use, licences, etc.\n- Data governance aspects: Categorisation of the data based on internal business requirements, e.g. public, business-relevant, personal\n- If applicable: categorisation into dependent (target variable, regressor) and independent (regressor) variables\n- ...\n\n#### Data Catalogue\nThe data catalogue basically represents an extended schema of a relational database.\n\n\n: Data catalogue for Dataset 1. {#tbl-data-catalogue}\n\n| Column index | Column name |  Datatype | Values (Range, validation rules) | Short description |\n|---|---|---|---|---|\n| 1 |   |   |   |   |\n| 2 |   |   |   |   |\n|   |   |   |   |   |\n\n\n#### If applicable: Entity Relationship Diagram\n\n\n#### Data Chracteristics and Quality\n\nThe methods of exploratory data analysis are used to describe relevant characteristics of the data and identify data quality issues. This includes, for example, \n\nthe univariate andalysis of numerical variables:\n\n* Number or fraction\n    * of unique values\n    * of missing values\n    * of zero values\n    * of negative values\n* Frequency distribution\n* Extreme values (minimum and maximum)\n* Histogram\n* Measures of central tendency:\n    * arithmetic mean\n    * median\n    * mode\n* Measures of dispersion and visualisation of dispersion:\n    * Range\n    * Quantiles and interquartile range (IQR)\n    * Box plot\n    * Variance and standard deviation \n\nand for categorical variables:\n\n* number of classes\n* and the corresponding distribution of values\n* fraction of missing values\n\nThe correlation between variables in the form of\n\n* correlation coefficients (e.g. Pearson, Spearman, Kendall)\n* and corresponding visualizations, e.g. scatter plot, heatmap, grouped box plot (for categorical-numerical relationships), ...\n\n\nThe results the exploratory data analysis should be summarised here (code and full output in the `eda` subfolder) and the implications for the subsequent data pre-processing, visualization design and implementation steps should be discussed. Is the data quality and quantity sufficient to achieve the visualisation goals? Is it necessary to acquire additional or different data? Are there any limitations that need to be considered in the design and implementation of the visualisation product? Are there any specific data characteristics that can be used to enhance the visualisation design? For example, if there are a lot of missing values in a dataset, this might need to be taken into account in the design of the visualisation product, e.g. by using specific visual encodings to indicate missing values or by providing options for filtering or imputing missing values. \n\nTip: `Ydata-profiling` is a Python library that can be used to perform exploratory data analysis and generate a comprehensive report on the characteristics of the data, including the distribution of values, missing values, correlations between variables, and more. This can be a useful tool to generate the full analysis in the code section and then only include the relevant information here in the data report.\n\nThis is an example on how to incorporate executable Python code and show a figure output:\n\n\n::: {#cell-fig-demo .cell execution_count=1}\n``` {.python .cell-code code-fold=\"true\"}\nimport matplotlib.pyplot as plt\n\n# Sample data\ncategories = ['Electronics', 'Clothing', 'Food', 'Books', 'Sports']\nrevenue = [42000, 28500, 35200, 12800, 19400]\n\n# Create the bar chart\nfig, ax = plt.subplots(figsize=(6, 3.75))\n\nbars = ax.bar(categories, revenue, color='#4C78A8', edgecolor='white')\n\nax.set_xlabel('Product Category')\nax.set_title('Monthly Revenue by Product Category')\n\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\nax.spines['left'].set_visible(False)\nax.yaxis.set_visible(False)\nax.tick_params(axis='x', length=0)\n\n# Add value labels on bars\nfor bar, val in zip(bars, revenue):\n    ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 500,\n            f'{val:,.0f}', ha='center', va='bottom', fontsize=9)\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![Distribution of monthly revenue across product categories (sample data).](data_report_files/figure-html/fig-demo-output-1.png){#fig-demo width=566 height=350}\n:::\n:::\n\n\nThen the figure can be referenced with @fig-demo.\n\n## Processed Data\n### Overview Processed Datasets\n\n: Overview of processed datasets used in the project. {#tbl-processed-overview}\n\n| Name | Quelle | Storage location |\n|----------------|-----------------------------------------|--------------------------------------------------------------------------|\n| Processed Dataset 1      | Name/short description of the data source | Link and/or short description of the location where the data is stored, e.g. accessible to the team |\n| Processed Dataset 2      | …                                       | …                                                                        |\n\n### Details Processed Dataset 1\n\n- Description of what information the dataset contains\n- Details and reasons for the processing steps -> Traceability and ensuring reproducibility\n- How can the data be accessed? Description, scripts, tools, ...\n- ...\n\n#### Data Catalogue\n\n\n#### If applicable: Entity Relationship Diagram\n\n\n#### Data Chracteristics and Quality\n\n\n### Details Processed Dataset 2\n...\n\n",
    "supporting": [
      "data_report_files"
    ],
    "filters": [],
    "includes": {}
  }
}